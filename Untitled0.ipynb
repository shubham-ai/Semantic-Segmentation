{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"oNjYN2wv2la7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"outputId":"c44592e9-a407-4256-f8e5-9e34a85028b1","executionInfo":{"status":"ok","timestamp":1540444993268,"user_tz":-330,"elapsed":32160,"user":{"displayName":"Shubham Sharma","photoUrl":"https://lh5.googleusercontent.com/-LY3_muO5rBw/AAAAAAAAAAI/AAAAAAAAdJc/Msit17XkAVg/s64/photo.jpg","userId":"07833073708161297456"}}},"cell_type":"code","source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"gMxBNsqoEwB6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"1d0324d0-9837-4dcd-b2ed-f6aa909b0a27","executionInfo":{"status":"ok","timestamp":1540446512993,"user_tz":-330,"elapsed":907,"user":{"displayName":"Shubham Sharma","photoUrl":"https://lh5.googleusercontent.com/-LY3_muO5rBw/AAAAAAAAAAI/AAAAAAAAdJc/Msit17XkAVg/s64/photo.jpg","userId":"07833073708161297456"}}},"cell_type":"code","source":["%cd 'drive/My Drive/udacity files/CarND-Semantic-Segmentation'\n","\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["[Errno 20] Not a directory: 'drive/'\n","/content/drive/My Drive/udacity files/CarND-Semantic-Segmentation\n"],"name":"stdout"}]},{"metadata":{"id":"3a9YfbSQvJ_P","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"Y4tpf3mkG_sQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":99},"outputId":"33d40152-c6c7-4120-e06f-627cfbf635c0","executionInfo":{"status":"ok","timestamp":1540446543873,"user_tz":-330,"elapsed":2766,"user":{"displayName":"Shubham Sharma","photoUrl":"https://lh5.googleusercontent.com/-LY3_muO5rBw/AAAAAAAAAAI/AAAAAAAAdJc/Msit17XkAVg/s64/photo.jpg","userId":"07833073708161297456"}}},"cell_type":"code","source":["# ! ssh-add ~./ssh/id_rsa\n","! ls -la"],"execution_count":39,"outputs":[{"output_type":"stream","text":["total 8\n","drwx------ 7 root root 4096 Oct 25 05:23 'My Drive'\n","drwx------ 2 root root 4096 Oct 25 05:23  .Trash\n"],"name":"stdout"}]},{"metadata":{"id":"HcFA5yeU0GnU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":383},"outputId":"52bf5f79-49e0-414e-9e4e-4823d53c6821","executionInfo":{"status":"ok","timestamp":1540130410026,"user_tz":-330,"elapsed":927393,"user":{"displayName":"Shubham Sharma","photoUrl":"","userId":"07833073708161297456"}}},"cell_type":"code","source":["|!0|import os.path\n","import tensorflow as tf\n","\n","import helper\n","\n","import warnings\n","from distutils.version import LooseVersion\n","import project_tests as tests\n","\n","\n","# Check TensorFlow Version\n","assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n","print('TensorFlow Version: {}'.format(tf.__version__))\n","\n","# Check for a GPU\n","if not tf.test.gpu_device_name():\n","    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n","else:\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n","def load_vgg(sess, vgg_path):\n","    \"\"\"\n","    Load Pretrained VGG Model into TensorFlow.\n","    :param sess: TensorFlow Session\n","    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n","    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n","    \"\"\"\n","    # TODO: Implement function\n","    #   Use tf.saved_model.loader.load to load the model and weights\n","    vgg_tag = 'vgg16'\n","    vgg_input_tensor_name = 'image_input:0'\n","    vgg_keep_prob_tensor_name = 'keep_prob:0'\n","    vgg_layer3_out_tensor_name = 'layer3_out:0'\n","    vgg_layer4_out_tensor_name = 'layer4_out:0'\n","    vgg_layer7_out_tensor_name = 'layer7_out:0'\n","    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n","\n","    graph = tf.get_default_graph()\n","    \n","    input_layer = graph.get_tensor_by_name(vgg_input_tensor_name)\n","    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n","    layer3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n","    layer4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n","    layer7_out = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n","    return input_layer, keep_prob, layer3, layer4, layer7_out\n","tests.test_load_vgg(load_vgg, tf)\n","\n","def conv_1x1(x,num_classes, k_size=(1,1), name='conv_1x1', strides=(1,1)):\n","    \"\"\"\n","    1x1 convolution \n","    \"\"\"\n","    with tf.name_scope(name):\n","        initializer = tf.random_normal_initializer(stddev = 0.001 )\n","\n","        conv_1x1_out = tf.layers.conv2d(x,num_classes,kernel_size=k_size, strides=strides, padding='same',kernel_initializer=initializer,\n","                             kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n","        tf.summary.histogram(name, conv_1x1_out)\n","\n","        return conv_1x1_out\n","    \n","def upsampling(x,num_classes,kernel_size=5, strides=2, name='upsample'):\n","    with tf.name_scope(name):\n","        initializer =  tf.random_normal_initializer(stddev=0.01)\n","        upsampling_out = tf.layers.conv2d_transpose(x,num_classes,kernel_size=kernel_size, strides=strides,padding = 'SAME', kernel_initializer = initializer,kernel_regularizer = tf.contrib.layers.l2_regularizer(1e-3))\n","        tf.summary.histogram(name, upsampling_out)\n","        return upsampling_out \n","def skip_layer(upsampling, convolution, name=\"skip_layer\"):\n","    with tf.name_scope(name):\n","        skip = tf.add(upsampling, convolution)\n","        tf.summary.histogram(name,skip)\n","        return skip\n","\n","def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n","    \"\"\"\n","    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n","    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n","    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n","    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n","    :param num_classes: Number of classes to classify\n","    :return: The Tensor for the last layer of output\n","    \"\"\"\n","    # TODO: Implement function \n","    # regularizer is imp to reuduce overfitting and penalize the weights\n","    # its presiving the spatial information \n","#     layer7a_out = tf.layers.conv2d(vgg_layer7_out, num_classes, 1,\n","#                 padding=\"same\",kernal_regularizer = tf.contrib.layers.l2_relugarizer(1e-3))\n","#     # deconvlution  \n","#     #upsampling\n","#     output = tf.layer.conv2d_transpose(conv_1x1, num_classes, 4, 2,\n","#                 padding = 'same', kernal_regularizer = tf.contrib.layers.l2_relugarizer(1e-3))\n","    \n","    layer7_1x1 = conv_1x1(vgg_layer7_out,num_classes)\n","    layer7_upsampling = upsampling(layer7_1x1,num_classes,4,2)\n","    layer4_1x1 = conv_1x1(vgg_layer4_out,num_classes)\n","    layer4_skip = skip_layer(layer7_upsampling,layer4_1x1)\n","    layer4_upsampling = upsampling(layer4_skip,num_classes,4,2)\n","    layer3_1x1 = conv_1x1(vgg_layer3_out,num_classes)\n","    layer3_skip= skip_layer(layer4_upsampling,layer3_1x1)\n","    output = upsampling(layer3_skip, num_classes,16,8)\n","    return output\n","tests.test_layers(layers)\n","\n","\n","def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n","    \"\"\"\n","    Build the TensorFLow loss and optimizer operations.\n","    :param nn_last_layer: TF Tensor of the last layer in the neural network\n","    :param correct_label: TF Placeholder for the correct label image\n","    :param learning_rate: TF Placeholder for the learning rate\n","    :param num_classes: Number of classes to classify\n","    :return: Tuple of (logits, train_op, cross_entropy_loss)\n","    \"\"\"\n","    # TODO: Implement function\n","    # classification loss in scene understaing\n","    # ligits will be flattining the image \n","    logits = tf.reshape(nn_last_layer,[-1, num_classes])\n","    labels = tf.reshape(correct_label,[-1,num_classes])\n","    corss_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label))\n","    \n","    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n","    train_op = optimizer.minimize(corss_entropy_loss)\n","#     softmax will do  Probablity \n","#     cross_entorpy (labels should be match with the size)\n","#     adms optimizer\n","    return logits,train_op,corss_entropy_loss\n","tests.test_optimize(optimize)\n","\n","\n","def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n","             correct_label, keep_prob, learning_rate):\n","    \"\"\"\n","    Train neural network and print out the loss during training.\n","    :param sess: TF Session\n","    :param epochs: Number of epochs\n","    :param batch_size: Batch size\n","    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n","    :param train_op: TF Operation to train the neural network\n","    :param cross_entropy_loss: TF Tensor for the amount of loss\n","    :param input_image: TF Placeholder for input images\n","    :param correct_label: TF Placeholder for label images\n","    :param keep_prob: TF Placeholder for dropout keep probability\n","    :param learning_rate: TF Placeholder for learning rate\n","    \"\"\"\n","    \n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    \n","    print(\"Training...\")\n","    print()\n","    for i in range(epochs):\n","        print(\"EPOCH {} ...\".format(i+1))\n","        for image, label in get_batches_fn(batch_size):\n","            _, loss = sess.run([train_op, cross_entropy_loss], \n","                               feed_dict={input_image: image, correct_label: label,                                keep_prob: 0.5, learning_rate: 0.0009})\n","            print(\"Loss: = {:.3f}\".format(loss))\n","        print()\n","tests.test_train_nn(train_nn)\n","\n","\n","def run():\n","    num_classes = 2\n","    image_shape = (160, 576)\n","    data_dir = \"./data\"\n","    runs_dir = './runs'\n","    tests.test_for_kitti_dataset(data_dir)\n","\n","    # Download pretrained vgg model\n","    helper.maybe_download_pretrained_vgg(data_dir)\n","\n","    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n","    # You'll nee  d a GPU with at least 10 teraFLOPS to train on.\n","    #  https://www.cityscapes-dataset.com/\n","\n","    with tf.Session() as sess:\n","        # Path to vgg model\n","        vgg_path = os.path.join(data_dir, 'vgg')\n","        # Create function to get batches\n","        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n","        correct_label = tf.placeholder(tf.int32,[None,None,None, num_classes],name=\"exact_label\")\n","        learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n","\n","        #layer from Vgg\n","        \n","        input_image, keep_prob, layer3_out, layer4_out, layer7_out= load_vgg(sess, vgg_path)\n","    \n","        \n","        #Creaste new layer\n","        layer_output = layers(layer3_out, layer4_out, layer7_out, num_classes)\n","        print(\"mighe be error\")\n","        # create loss and optimizer operations.\n","        logits, train_op, corss_entropy_loss = optimize(layer_output,correct_label, learning_rate, num_classes)\n","                        \n","        # OPTIONAL: Augment Images for better results\n","        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n","\n","        # TODO: Train NN using the train_nn function\n","        epochs=51\n","\n","                              \n","        batch_size = 5\n","                              \n","        saver = tf.train.Saver()\n","        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, corss_entropy_loss, input_image, correct_label, keep_prob, learning_rate)\n","        \n","        \n","        # TODO: Save inference data using helper.save_inference_samples\n","        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n","\n","        # OPTIONAL: Apply the trained model to a video\n","\n","\n","if __name__ == '__main__':\n","    run()\n","\n","\n","\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["TensorFlow Version: 1.12.0-rc1\n","Default GPU Device: /device:GPU:0\n","Tests Passed\n","Tests Passed\n","WARNING:tensorflow:From <ipython-input-3-2ae779e00805>:117: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","Tests Passed\n","INFO:tensorflow:Restoring parameters from ./data/vgg/variables/variables\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if issubdtype(ts, int):\n","/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n","  elif issubdtype(type(size), float):\n"],"name":"stderr"}]},{"metadata":{"id":"rOtA8UBWuF2-","colab_type":"code","colab":{}},"cell_type":"code","source":["-"],"execution_count":0,"outputs":[]}]}